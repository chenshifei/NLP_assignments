% !TEX program = xelatex
% !BIB program = bibtex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LATEX FORMATTING - LEAVE AS IS %%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article} % documenttype: article
\usepackage[top=20mm,left=20mm,right=20mm,bottom=15mm,headsep=15pt,footskip=15pt,a4paper]{geometry} % customize margins
\usepackage{xeCJK}
\usepackage[inline]{enumitem}
\usepackage{url}
\usepackage{apalike}

\makeatletter
\def\@maketitle{   % custom maketitle 
\begin{center}
{\bfseries \@title}
{\bfseries \@author}
{\@date}
\end{center}
\smallskip \hrule \bigskip }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% MAKE CHANGES HERE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{{\LARGE Chinese Language under Censorship}\\[1.5mm]} % Replace 'X' by number of Assignment
\author{Shifei Chen\\} % Replace 'Firstname Lastname' by your name.
\date{5LN710, Natural Languages Processing, Uppsala University}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% BEGIN DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% From here on, edit document. Use sections, subsections, etc.
%%% to structure your answers.
\begin{document}
\maketitle

\section{Introduction}

In 2017, 64\% of netizens lived in countries where their freedom of expression is supressed and for the third consecutive year, China was the world's worst abuser of internet freedom \footnote{\url{https://freedomhouse.org/report/freedom-net/freedom-net-2017}}. The censorship system in China behaves in several ways including limited access to websites and keywords, removal of posts on social network websites and blogs, cyber attacks on websites and createing contents for public opinions manipulation. The system works behind th curtain so its organization and mechanism are still not clear to the public. Censored contents usually contain information about pornography, violence and last but least, information against the communist regime. But this doesn't mean all negative information will be censored eventually, there are plenty of them that are considered to be ``safe'' being expressed freely on the internet. Also, some of the contents that are obfuscated intentially can escape. To understand what kind of expression are more likely to be supressed from a linguistic prospective and how to avoid it by encoding languages, I have conducted this survey base on the First Workshop on Natural Language Processing for Internet Freedom \cite{brew2018proceedings}.

\section{Characteristics of Sensitive Words Blacklists}

Censorship in China isn't implemented by the governments or its subordinaries. Service providers are required to comply with content regulations by themselves, which means tech companies and individual developers are liable for the content generated by their users. A simple solution is filtering unfavored information by using a balcklist, which isn't a novelty to block out contents and it is being widely carried out by tech companies and governments around the world, such as Google \footnote{\url{https://en.wikipedia.org/wiki/Censorship_by_Google}} and IWF \footnote{\url{https://www.iwf.org.uk/news/iwf-response-to-prime-ministers-statement}}.

Reverse engineering revealed some blacklists from popular Chinese mobile phone applicaitons \cite{knockel2011three} \cite{knockel2015every} and games \cite{knockel2017measuring}. These studies suggest that, ``rather than Chinese censorship being top-down and monolithic, it is a decentralized system where developers are liable for deciding what to censor themselves'' \cite{knockel2018effect} since the blacklists found have very little overlap between each other.

If it can be inferred that these big companies have compiled their own blacklists of sensitive words independently since they have the resource to do so, then individual developers are supposed to have a common blacklist as its length are often at tens of thousands level. To find out that, \cite{knockel2018effect} scrapped suspected blacklist files from Chinese open source projects hosted on Github and used machine-learning techniques to determine whether they are blacklists or not.

\subsection{Scrapping Blacklists and Extract Their Contents}

They used 21924 unique keywords which were found blacklisted by some popular apps and games, combined them with some other common blacklist related words appeared in the file name or the content like ``dirty'', ``sensitive'' and ``forbid'', all of which were observed from previous works, and searched them together on Github to get possible blacklist keywords files from all of the open source projects.

Then they developed a string extractor to extract lists of strings from the file. It supports most-chosen file formats for blacklists by developers such as XML, JSON and CSV, plus C-like source code and plaintexts. However it didn't support SQL database files as their grammar is not consistent.

\subsection{Blacklists Classification}

To futher classify if the list is a true Chinese blacklist, they used both anaive approach and a machine-learning approach. In the naive approach, researchers simply counts any list that contains ``法轮''(Falun Gong) as a blacklist as this word appears in every blacklist publicly known. The limitation of the naive approach is also obvious since it can only find blacklists containing ``法轮''. Even though by far it appears in every blacklist, it can not be a guranteened feature of every possible Chinese blacklist.

In the machine-learning approach, they ``used a one-class support vector machine (SVM) \cite{scholkopf2001estimating}, as implemented by Scikit-learn \cite{pedregosa2011scikit} and LIBSVM \cite{chang2011libsvm}.'' The classifier requires to be trained by only one class yet maintain the ability to classify both the positive and the negative results of that class. The training data was the Chinese blacklists that were reverse engineered from popular apps, plus a blacklist from Google \footnote{\url{https://caiguanhao.wordpress.com/2012/06/01/google-gfw-blacklist}}.

They modelled their possible blacklists to be ``vectors of counts of the number of occurrences of each Chinese word in that list.''\cite{knockel2018effect} In addition, to make the calculation tractable, they decided to apply singular-value decomposition to reduce the dimensinality of the dataset. After comparing with the golden standard, which is their result from the naive approach, the final dimension is 46.

\subsection{Result}

After manual verification, the naive approach returned 884 Chinese blacklists while the machine-learning approach found 1054. The longest blacklist had 38237 words and the mean length and the median length was 2128 and 1026, respectivelly.

These 1054 blacklists contains a wide range of words including erotica, Falun Gong reference, possible independence movements, criticisms to the 
government, political leader names, etc.

The inter-similarity between blacklists, measured by Jaccard similarity and $max(\% $ of $x$ in $y$, $\%$ of $y$ in $x)$ for list $x$ and $y$, were very low. It indicates that there was very little overlap between each blacklist. It corresponds to the same discovery from the blacklists in popular Chinese applicaitons and games. Although how so many individual developers can compose blacklists on their own independently remains unknown, again, both the result from this study and the previous ones had proved that the censorship system in China is a decentralized one.

\section{Characteristics of Censorable Language Beyond Blacklists}

So far we have covered the first form of censorship in China, sensitive keywords blacklist. But sensitive words does not always get removed, some of them can survive. Moreover, sometimes sentences containing no blacklisted words can also be censored. In order to have strict controls over their user-generated contents, major social network services such as Sina Weibo, has incoporated manual checking in addition to blacklists to make the decision of whether to delete a post or not. So what kind of words will be removed besides those on the blacklsit already? \cite{ng2018linguistic} answered this question by explore the linguistic characteristics on censored and uncensored posts from Sina Weibo which contains the same sensitive words. Based on the discoveries from Psychology that rejecting information takes more efforts for human beings than accpeting it\cite{lewandowsky2012misinformation}, and people's opinion towards are unstable overtime and contexts\cite{xu2008thinking}, their hypothesis were, \begin {enumerate*}[label=\emph{\alph*})]
\item uncensored contents are easier to digest and \item the linguistic characteristics of censored and uncensored contents are different. 
\end {enumerate*}

\subsection{Corpus}

The corpus consists of both censored and uncensored posts on Sina Weibo whose topic are scandles happened or happening in China. It is inspired by the Grass-Mud-Horse Lexicon, which is widely used by Chinese netizens in their sarcasms and was proved to be relavent and significant by previous studies such as \cite{tang2011symbolic} \cite{wang2012china}. The team chose words form the following four topics,
\begin {enumerate*}[label=\emph{\alph*})]
\item pollution and food safety, \item internet censorship and propaganda, \item Bo Xilai and \item kindergarten abuse.
\end {enumerate*}
All of the uncensored posts came from search results from Sina Weibo itself using the keywords from the lexicon, while censored posts were searched on FreeWeibo and WeiboScope as they track posts disappeared on Sina Weibo.

All together they have collected 1023 censored posts and 1138 uncensored ones from the four topics above, segmented by Jieba before futher experiments.

\subsection{Extract Features}

Kei and his team extract several features to measure the readability of a Weibo post, such as character frequency, word frequency, sentiment (only shows if the post is positive or not), sentiment classes, etc. Some of the features are unique to Chinese such as idioms, where they have also discoveried that the more idioms in a post the harder it is to understand. Finally, there are also some composite features like Readability 1 ("the mean of character frequency, word frequency and word count to semantic groups ratio", the lower the score is, the higher the readability is) \cite{ng2018linguistic}.

To reveal more details about the linguistic characteristics, the team in addition used word embeddings as one of the feature. They computed a 200 dimensional vector for each word in a post as its representive in the large word vector space trained by 30000 latest Chinese Wikipedia aritcles. Then they did eigen decomposition for the 200*200 covarience matrix for each post. The result eigenvectors are "the directions in which the data varies the most" and the last 40 of them were used as the feature in order to reach calculation tractability since they have covered 85\% of total varience.

All of these features were put into both a Naive Bayes classifier and a Support Vector Machine one to classify censored posts or not. 

\subsection{Result}

Overall the accuracy of both the Naive Bayes classifier and the SMO classifier achieved around 0.7 in all four categories, with Readability 1 being the best performing feature. Also, the average readability score of the censored posts is lower than then uncensored ones. This suggests that readability plays an important role in censors decision as writes tend to use more uncommon words and less straight foward expressions to evade censorship. This also supports the hypothesis that uncensored contents are easier to read.

Another discovery from the experiment found that word express strong opinions, such as swear words and words that contains anger, are more likely to be censored. In other words, posts that have more potential to collective actions and social engagement are considered to be more dangerous among censors. Casual discussion on the current state of scandles survives more. By giving such an outlet to express, negative sentiment are less possible to grow into anger and leads to social movements.

\section{Lanugage Encoding to Avoid Censorship}

Besides studies focused on determining what kind of language are more exposed to the danger of censorship, some other studies such as the one \cite{ji2018creative} did tried to overview possible solutions to avoid censorship from a linguistic prospective.

For human beings encoding a language can be approached by a variety of ways. Most of them is domain-mapping, that is by mapping an entity to another entity in a different domain which is consistent and easy to remember. 

Another higher level technique is story encoding. For example a post describing the struggle between the officials for the arrest of Bo Xilai goes ``A few days ago, Beijing was hosting an innovative \textbf{tug-of-war} for the \textbf{elderly}; \textbf{this game} has \textbf{nine contestants} in all. \textbf{The first round} of the contest is still intense ...\textbf{The teletubby team} noticeably has the advantage and, relatively, \textbf{the Master Kang team} is obviously falling short.''

All of the text in bold above refers to encoded entity names and the story it self is also a metaphor. The post also revealed three challenges for decoding.
\begin {enumerate*}[label=\emph{\alph*})]
\item synonymy and polysemy,  \item large number of candidates and \item lack of background knowledge for the target concepts and stories.
\end {enumerate*}

\subsection{System Encoding and Decoding}

Reaching the lower level language encoding techniques are already proved to be feasible to computers. \cite{zhang2014appropriate} and \cite{zhang2015context} developed several approaches to automatically encode entities including Phonetic Substitution, Spelling Decomposition, Nickname Generation, Translation and Transliteration and Historical Figure Mapping. For encoding longer texts, using a simple cipher can encode and create languages like Leet and Martian script, just like encryption. Moreover, natural language generation is applied to encoding in these three ways.
\begin {enumerate*}[label=\emph{\alph*})]
\item portmanteau neologism creation, \item dynamic phrasebooks and \item poetry passwords.
\end {enumerate*}

Portmanteau neologism creation \cite{deri2015make} is creating new words such as mixing \textit{friend} and \textit{enemy} to \textit{frenemy}. It can not be processed by machines yet and it requires a carefully picked spelling and a fusion on the phonetic level.

Next, dynamic phrasebooks \cite{shi2014speak} is something appears on the crack guide of speaking a new language for tourists. They need to know nothing of the new language but by imitating the pronounciation of the new language using phonetic spellings in another language that they already know, e.g saying \textit{Good morning} by pronounce the Chinese words \textit{古德莫宁} since they have similar sounds to non-native speakers.

Finally, poetry passwords \cite{greene2010automatic} refers to an old technique of remembering long passwords/numbers--making a poem for it. We could imagine that by applying it in the opposite way, we could generate long passwords and use them instead of the words we are supposed to spread.

\section{Summary}

Censorship is a Natural Language Processing application gaining more and more focus on, especially with the rising concern about the degenerating internet freedom over the years.

People studied the linguistic characteristics of censored languages to understand the mechanism and the structure of the censorship system, which corresponds to many previous studies from other disciplines like Psychology, Computer Science.

On the other hand, Natural Language Generation tries to propose methods for people to evade such kind of censorship. It works just like the opposite of Word Sense Disambiguation--We intentially make the word sense vague and hard to decode for outsiders in order to preserve communicating any kind of information freely. People have already made some initial process on it.

Since China is the worst abuser of internet freedom there are lots of works focused on Chinese language specifically. But they all have the same limitation that there aren't too much living corpus available since the censorship system is built to remove unfavored information from day one. Also, not only the language is envolving with time, that censorship system is also improving itself at the same time. We should all bear that in mind.

\bibliographystyle{apalike}
\bibliography{shifei_chen_survey.bib}

\end{document}
